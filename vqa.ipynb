{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "visual_question_answering.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rx1hCXfRmf8j",
        "bin3NnlohuUz",
        "576rl4Re--Lu",
        "kC8xUPmHqkT2",
        "zw9N3gGTubbq",
        "g0Lwfj737xsW",
        "0gN0wNbCms_b",
        "sjvDzU8aBR29",
        "bklQ3oDt_hsg",
        "ESZZKbo1_pTU",
        "w1gBud2qBpC6"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx1hCXfRmf8j"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_QYgDdA7e9y"
      },
      "source": [
        "import os, shutil, argparse\n",
        "\n",
        "from datetime import datetime\n",
        "import random\n",
        "import pathlib\n",
        "import cv2, spacy, numpy as np\n",
        "import re\n",
        "import gc\n",
        "import csv\n",
        "import glob\n",
        "import time\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from imutils import paths\n",
        "import math\n",
        "import numpy as np\n",
        "import pickle\n",
        "import operator\n",
        "from operator import itemgetter\n",
        "from itertools import zip_longest\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU, add, Conv2D, Reshape\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
        "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D, multiply\n",
        "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import VGG16, VGG19\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing import image, text, sequence\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow_addons.metrics import F1Score\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4Xoh5zTfCov"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "colab = True\n",
        "if not colab:\n",
        "  physical_devices = tf.config.list_physical_devices('GPU') \n",
        "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "  print(physical_devices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bin3NnlohuUz"
      },
      "source": [
        "### Load files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AsgEp8tfuy_",
        "outputId": "03c98e77-3b95-4f82-b8b0-0b2332ce997a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "dataset_dir = pathlib.Path('/content/VQA_Dataset/')\n",
        "\n",
        "if not os.path.isdir(dataset_dir) or len(os.listdir(dataset_dir)) == 0:\n",
        "  print(\"Unzipping files... \", end='')\n",
        "  !unzip /content/drive/MyDrive/anndl-2020-vqa.zip -d /content/ > /dev/null\n",
        "  print(\"done\")\n",
        "\n",
        "questions_json = dataset_dir / \"train_questions_annotations.json\"\n",
        "test_json = dataset_dir / \"test_questions.json\"\n",
        "images_dir = dataset_dir / \"Images\"\n",
        "\n",
        "if os.path.exists(questions_json):\n",
        "  os.rename(questions_json, dataset_dir / \"questions_annotations.json\")\n",
        "questions_json = dataset_dir / \"questions_annotations.json\"\n",
        "\n",
        "!du -h /content/VQA_Dataset/*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "Unzipping files... done\n",
            "4.1G\t/content/VQA_Dataset/Images\n",
            "5.4M\t/content/VQA_Dataset/questions_annotations.json\n",
            "476K\t/content/VQA_Dataset/test_questions.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFuS2QuDqYGE"
      },
      "source": [
        "# Extract Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "576rl4Re--Lu"
      },
      "source": [
        "### Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je9HdCe_fsjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "699cc527-5072-4e10-ebba-667c0457574f"
      },
      "source": [
        "def process_question_annotation(json_file):\n",
        "  images_dict = json.load(open(json_file, 'r'))\n",
        "  \n",
        "  images_data = []\n",
        "  for key, value in tqdm(images_dict.items()):\n",
        "    image_path = images_dir / (value['image_id']+'.png') \n",
        "    question = value['question']\n",
        "    answer =  value['answer']\n",
        "    images_data.append({'img_path': str(image_path), 'question': question, 'answer': answer})\n",
        "\n",
        "  target_file = json_file.parent / (\"processed_\" + json_file.name)\n",
        "  json.dump(images_data, open(target_file, 'w+'))\n",
        "\n",
        "  return target_file\n",
        "\n",
        "processed_questions_json = process_question_annotation(questions_json)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 58832/58832 [00:00<00:00, 178437.91it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0QZwhGPmxgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87877d8f-d554-45e7-98e3-46e02f040e43"
      },
      "source": [
        "print(\"\\n%d images found in the dataset.\" % len(os.listdir(images_dir)))\n",
        "\n",
        "train_dataset = json.load(open(processed_questions_json, 'r'))\n",
        "print(\"%d questions loaded.\" % len(train_dataset))\n",
        "\n",
        "answers_occurrences = defaultdict(int)\n",
        "for answer in list(map(itemgetter('answer'), train_dataset)):\n",
        "\t\tanswers_occurrences[answer] += 1\n",
        "num_answers = len(answers_occurrences)\n",
        "print('%d possible answers found.' % num_answers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "29333 images found in the dataset.\n",
            "58832 questions loaded.\n",
            "58 possible answers found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk-hKqicnxlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff1529ee-5d01-4076-d5d3-ee22583947ca"
      },
      "source": [
        "def selectTopAnswersData(questions_list, answers_list, images_list, k):\n",
        "  \"\"\"\n",
        "  Reduces the train dataset to contain only the datapoints whose answer occurence is in top k \n",
        "\n",
        "  Input:\n",
        "    questions_list: list of questions\n",
        "    answer_list: list of most frequent answer\n",
        "    answers_list: list of answers \n",
        "    images_list: list of image path\n",
        "    k: number of top answers \n",
        "\n",
        "  Returns:\n",
        "    Returns tuple of reduced dataset and the top k answers (questions_list, answer_list, answers_list, images_list, top_answers)\n",
        "  \"\"\"\n",
        "  answers_occurrences = defaultdict(int)\n",
        "\n",
        "  for answer in answers_list:\n",
        "    answers_occurrences[answer] += 1\n",
        "\n",
        "  sorted_occurrences = sorted(answers_occurrences.items(), key=operator.itemgetter(1), reverse=True)[0: k]\n",
        "  top_answers = [ans for ans, _ in sorted_occurrences]\n",
        "\n",
        "  new_questions_list = []\n",
        "  new_answers_list = []\n",
        "  new_images_list = []\n",
        "\n",
        "  for question, answer, image in zip(questions_list, answers_list, images_list):\n",
        "    if answer in top_answers:\n",
        "      new_questions_list.append(question)\n",
        "      new_answers_list.append(answer)\n",
        "      new_images_list.append(image)\n",
        "    \n",
        "  print('Dataset size reduced by', np.round(((len(questions_list) - len(new_questions_list)) / len(questions_list)) * 100, 2),'%')\n",
        "  \n",
        "  return (new_questions_list, new_answers_list, new_images_list, top_answers)\n",
        "\n",
        "questions_train, answers_train, images_train, top_answers = selectTopAnswersData(list(map(itemgetter('question'), train_dataset)), \n",
        "                                                                                list(map(itemgetter('answer'), train_dataset)), \n",
        "                                                                                list(map(itemgetter('img_path'), train_dataset)),\n",
        "                                                                                num_answers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size reduced by 0.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC8xUPmHqkT2"
      },
      "source": [
        "### Images features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmYPLgiSqbP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd127cd9-29ab-4ed9-ca28-8b9e5aaa060a"
      },
      "source": [
        "IMG_SIZE = (350, 200)\n",
        "BATCH_SIZE = 300\n",
        "CHANNELS_FIRST = False\n",
        "\n",
        "def image_feature_extractor(images_dir, clear_existing=False):\n",
        "\n",
        "  image_list = os.listdir(images_dir)\n",
        "  target_path = images_dir.parent / \"features\"\n",
        "  if not os.path.exists(target_path):\n",
        "    os.mkdir(target_path)\n",
        "  \n",
        "  if not clear_existing and len(os.listdir(target_path)) > 0:\n",
        "    return target_path\n",
        "\n",
        "  for f in os.listdir(target_path):\n",
        "    os.remove(target_path / f)\n",
        "  \n",
        "  if CHANNELS_FIRST:\n",
        "    K.set_image_data_format('channels_first')\n",
        "    input_tensor = Input(shape=(3, IMG_SIZE[1], IMG_SIZE[0]))\n",
        "  else:\n",
        "    K.set_image_data_format('channels_last')\n",
        "    input_tensor = Input(shape=(IMG_SIZE[1], IMG_SIZE[0], 3))\n",
        "  \n",
        "  model = VGG19(weights=\"imagenet\", include_top=False, input_tensor=input_tensor)\n",
        "\n",
        "  progbar = utils.Progbar(int(np.ceil(len(image_list) / float(BATCH_SIZE))))\n",
        "  progbar.update(0)\n",
        "\n",
        "  for (b, i) in enumerate(range(0, len(image_list), BATCH_SIZE)):\n",
        "\t\t\n",
        "    batch_range = range(i, min(i + BATCH_SIZE, len(image_list)))\n",
        "    batchPaths = image_list[batch_range[0]: batch_range[-1]+1]\n",
        "\n",
        "    batchImages = []\n",
        "    batchIds = []\n",
        "    for imagePath in batchPaths:\n",
        "\n",
        "      img = image.load_img(str(images_dir / imagePath), target_size=IMG_SIZE[::-1])\n",
        "      img = image.img_to_array(img)\n",
        "\n",
        "      img = np.expand_dims(img, axis=0)\n",
        "      img = preprocess_input(img)\n",
        "    \n",
        "      batchImages.append(img)\n",
        "      batchIds.append(imagePath.split('.')[0][-6:])\n",
        "\n",
        "    batchImages = np.vstack(batchImages) \n",
        "\n",
        "    features = model.predict(batchImages) \n",
        "    features = tf.reshape(features, (features.shape[0],-1, features.shape[3])) \n",
        "\n",
        "    for id, feat in zip(batchIds, features):\n",
        "      np.save(os.path.join(target_path, id), feat)\n",
        "    \n",
        "    progbar.update(b+1)\n",
        "  \n",
        "  return target_path\n",
        "\n",
        "features_dir = image_feature_extractor(images_dir, clear_existing=False)\n",
        "print(\"%d images processed.\" % len(os.listdir(features_dir)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "98/98 [==============================] - 531s 5s/step\n",
            "29333 images processed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw9N3gGTubbq"
      },
      "source": [
        "### Questions features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFQaYn5WueSJ"
      },
      "source": [
        "import re\n",
        "\n",
        "def process_sentence(sentence):\n",
        "  periodStrip  = re.compile(\"(?!<=\\d)(\\.)(?!\\d)\")\n",
        "  commaStrip   = re.compile(\"(\\d)(\\,)(\\d)\")\n",
        "  punct        = [';', r\"/\", '[', ']', '\"', '{', '}',\n",
        "                  '(', ')', '=', '+', '\\\\', '_', '-',\n",
        "                  '>', '<', '@', '`', ',', '?', '!']\n",
        "  contractions = {\"aint\": \"ain't\", \"arent\": \"aren't\", \"cant\": \"can't\", \"couldve\": \"could've\", \"couldnt\": \"couldn't\", \\\n",
        "                  \"couldn'tve\": \"couldn't've\", \"couldnt've\": \"couldn't've\", \"didnt\": \"didn't\", \"doesnt\": \"doesn't\", \"dont\": \"don't\", \"hadnt\": \"hadn't\", \\\n",
        "                  \"hadnt've\": \"hadn't've\", \"hadn'tve\": \"hadn't've\", \"hasnt\": \"hasn't\", \"havent\": \"haven't\", \"hed\": \"he'd\", \"hed've\": \"he'd've\", \\\n",
        "                  \"he'dve\": \"he'd've\", \"hes\": \"he's\", \"howd\": \"how'd\", \"howll\": \"how'll\", \"hows\": \"how's\", \"Id've\": \"I'd've\", \"I'dve\": \"I'd've\", \\\n",
        "                  \"Im\": \"I'm\", \"Ive\": \"I've\", \"isnt\": \"isn't\", \"itd\": \"it'd\", \"itd've\": \"it'd've\", \"it'dve\": \"it'd've\", \"itll\": \"it'll\", \"let's\": \"let's\", \\\n",
        "                  \"maam\": \"ma'am\", \"mightnt\": \"mightn't\", \"mightnt've\": \"mightn't've\", \"mightn'tve\": \"mightn't've\", \"mightve\": \"might've\", \\\n",
        "                  \"mustnt\": \"mustn't\", \"mustve\": \"must've\", \"neednt\": \"needn't\", \"notve\": \"not've\", \"oclock\": \"o'clock\", \"oughtnt\": \"oughtn't\", \\\n",
        "                  \"ow's'at\": \"'ow's'at\", \"'ows'at\": \"'ow's'at\", \"'ow'sat\": \"'ow's'at\", \"shant\": \"shan't\", \"shed've\": \"she'd've\", \"she'dve\": \"she'd've\", \\\n",
        "                  \"she's\": \"she's\", \"shouldve\": \"should've\", \"shouldnt\": \"shouldn't\", \"shouldnt've\": \"shouldn't've\", \"shouldn'tve\": \"shouldn't've\", \\\n",
        "                  \"somebody'd\": \"somebodyd\", \"somebodyd've\": \"somebody'd've\", \"somebody'dve\": \"somebody'd've\", \"somebodyll\": \"somebody'll\", \\\n",
        "                  \"somebodys\": \"somebody's\", \"someoned\": \"someone'd\", \"someoned've\": \"someone'd've\", \"someone'dve\": \"someone'd've\", \\\n",
        "                  \"someonell\": \"someone'll\", \"someones\": \"someone's\", \"somethingd\": \"something'd\", \"somethingd've\": \"something'd've\", \\\n",
        "                  \"something'dve\": \"something'd've\", \"somethingll\": \"something'll\", \"thats\": \"that's\", \"thered\": \"there'd\", \"thered've\": \"there'd've\", \\\n",
        "                  \"there'dve\": \"there'd've\", \"therere\": \"there're\", \"theres\": \"there's\", \"theyd\": \"they'd\", \"theyd've\": \"they'd've\", \\\n",
        "                  \"they'dve\": \"they'd've\", \"theyll\": \"they'll\", \"theyre\": \"they're\", \"theyve\": \"they've\", \"twas\": \"'twas\", \"wasnt\": \"wasn't\", \\\n",
        "                  \"wed've\": \"we'd've\", \"we'dve\": \"we'd've\", \"weve\": \"we've\", \"werent\": \"weren't\", \"whatll\": \"what'll\", \"whatre\": \"what're\", \\\n",
        "                  \"whats\": \"what's\", \"whatve\": \"what've\", \"whens\": \"when's\", \"whered\": \"where'd\", \"wheres\": \"where's\", \"whereve\": \"where've\", \\\n",
        "                  \"whod\": \"who'd\", \"whod've\": \"who'd've\", \"who'dve\": \"who'd've\", \"wholl\": \"who'll\", \"whos\": \"who's\", \"whove\": \"who've\", \"whyll\": \"why'll\", \\\n",
        "                  \"whyre\": \"why're\", \"whys\": \"why's\", \"wont\": \"won't\", \"wouldve\": \"would've\", \"wouldnt\": \"wouldn't\", \"wouldnt've\": \"wouldn't've\", \\\n",
        "                  \"wouldn'tve\": \"wouldn't've\", \"yall\": \"y'all\", \"yall'll\": \"y'all'll\", \"y'allll\": \"y'all'll\", \"yall'd've\": \"y'all'd've\", \\\n",
        "                  \"y'alld've\": \"y'all'd've\", \"y'all'dve\": \"y'all'd've\", \"youd\": \"you'd\", \"youd've\": \"you'd've\", \"you'dve\": \"you'd've\", \\\n",
        "                  \"youll\": \"you'll\", \"youre\": \"you're\", \"youve\": \"you've\"}\n",
        "\n",
        "  inText = sentence.replace('\\n', ' ')\n",
        "  inText = inText.replace('\\t', ' ')\n",
        "  inText = inText.strip()\n",
        "  outText = inText\n",
        "  for p in punct:\n",
        "    if (p + ' ' in inText or ' ' + p in inText) or (re.search(commaStrip, inText) != None):\n",
        "      outText = outText.replace(p, '')\n",
        "    else:\n",
        "      outText = outText.replace(p, ' ')\n",
        "  outText = periodStrip.sub(\"\", outText, re.UNICODE)\n",
        "  outText = outText.lower().split()\n",
        "  for wordId, word in enumerate(outText):\n",
        "    if word in contractions:\n",
        "      outText[wordId] = contractions[word]\n",
        "  outText = ' '.join(outText)\n",
        "  return outText"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da_U_SMa5V3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9ef6e44-62a0-4d30-88c4-b5dfcf3d5eff"
      },
      "source": [
        "questions_train_processed = pd.Series(questions_train).apply(process_sentence)\n",
        "print(questions_train_processed)\n",
        "\n",
        "tok=text.Tokenizer(filters='')\n",
        "tok.fit_on_texts(questions_train_processed)\n",
        "\n",
        "question_data_train = tok.texts_to_sequences(questions_train_processed)\n",
        "\n",
        "questions_len = [len(text) for text in question_data_train]\n",
        "MAX_QUESTIONS_LEN = max(questions_len)\n",
        "print(\"Maximum questions length:\", MAX_QUESTIONS_LEN)\n",
        "\n",
        "question_data_train=sequence.pad_sequences(question_data_train, maxlen=MAX_QUESTIONS_LEN, padding='post')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0                       who looks happier\n",
            "1              where is the woman sitting\n",
            "2                where is the man sitting\n",
            "3                      is this man hungry\n",
            "4             who is holding the football\n",
            "                       ...               \n",
            "58827                 what animal is that\n",
            "58828    is there a fire in the fireplace\n",
            "58829        how many pillows on the sofa\n",
            "58830           how many people are there\n",
            "58831               what is on the pillow\n",
            "Length: 58832, dtype: object\n",
            "Maximum questions length: 21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO9Aex5N6IQR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "outputId": "b26a5341-bab3-4c72-8b45-3a4392a12072"
      },
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "sns.distplot(questions_len, color='red')\n",
        "plt.title('Distribution of Question length')\n",
        "plt.xlabel('Length of Question')\n",
        "plt.ylabel('Question count')\n",
        "plt.xlim(0, 30)\n",
        "plt.show()\n",
        "\n",
        "for i in range(0,9):\n",
        "    print(10*i,'percentile value is', np.percentile(questions_len,10*i))\n",
        "\n",
        "for i in range(0,11):\n",
        "    print(90+i,'percentile value is',np.percentile(questions_len,90+i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0 percentile value is 2.0\n",
            "10 percentile value is 4.0\n",
            "20 percentile value is 5.0\n",
            "30 percentile value is 5.0\n",
            "40 percentile value is 5.0\n",
            "50 percentile value is 6.0\n",
            "60 percentile value is 6.0\n",
            "70 percentile value is 7.0\n",
            "80 percentile value is 8.0\n",
            "90 percentile value is 9.0\n",
            "91 percentile value is 9.0\n",
            "92 percentile value is 9.0\n",
            "93 percentile value is 9.0\n",
            "94 percentile value is 10.0\n",
            "95 percentile value is 10.0\n",
            "96 percentile value is 10.0\n",
            "97 percentile value is 11.0\n",
            "98 percentile value is 11.0\n",
            "99 percentile value is 13.0\n",
            "100 percentile value is 21.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0Lwfj737xsW"
      },
      "source": [
        "### Answers features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpx94OsG70kl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "704ee1be-fda8-4487-9739-735221762611"
      },
      "source": [
        "labelencoder = preprocessing.LabelEncoder()\n",
        "labelencoder.fit(answers_train)\n",
        "\n",
        "print(\"%d answers classes found.\" % len(labelencoder.classes_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58 answers classes found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gN0wNbCms_b"
      },
      "source": [
        "# Create Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGow-Tky7mfo"
      },
      "source": [
        "def get_answers_matrix(answers, encoder):\n",
        "\n",
        "\ty = encoder.transform(answers) \n",
        "\tnb_classes = encoder.classes_.shape[0]\n",
        "\tY = utils.to_categorical(y, nb_classes)\n",
        "\treturn Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRP4vnDU5NAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d70ae7d-2958-4eb0-9fdc-bb62d22bce46"
      },
      "source": [
        "sss = StratifiedShuffleSplit(n_splits=1, test_size= 0.25,random_state=42)\n",
        "\n",
        "TRAIN_INDEX, VAL_INDEX = list(sss.split(images_train, answers_train))[-1]\n",
        "\n",
        "images_list_tr, images_list_val = np.array(images_train)[TRAIN_INDEX.astype(int)], np.array(images_train)[VAL_INDEX.astype(int)]\n",
        "print(\"Images splitted: %d for training, %d for validation.\" % (len(images_list_tr), len(images_list_val)))\n",
        "\n",
        "questions_tr, questions_val = question_data_train[TRAIN_INDEX], question_data_train[VAL_INDEX]\n",
        "print(\"Questions splitted: %d for training, %d for validation.\" % (len(questions_tr), len(questions_val)))\n",
        "\n",
        "answers_matrix = get_answers_matrix(answers_train, labelencoder)\n",
        "answers_tr, answers_val = answers_matrix[TRAIN_INDEX], answers_matrix[VAL_INDEX]\n",
        "print(\"Answers splitted: %d for training, %d for validation.\" % (len(answers_tr), len(answers_val)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Images splitted: 44124 for training, 14708 for validation.\n",
            "Questions splitted: 44124 for training, 14708 for validation.\n",
            "Answers splitted: 44124 for training, 14708 for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn_tBrf0-uv8"
      },
      "source": [
        "def map_func(img_name, ques, ans):\n",
        "    img_name = img_name.decode(\"utf-8\")\n",
        "    img_path = img_name.replace('Images','features')\n",
        "    img_path = img_path.replace('png','npy')\n",
        "    img_tensor = np.load(img_path)\n",
        "    return img_tensor, ques, ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr-vajOG_MHG"
      },
      "source": [
        "BUFFER_SIZE = 5000\n",
        "\n",
        "dataset_tr = tf.data.Dataset.from_tensor_slices((images_list_tr, questions_tr, answers_tr))\n",
        "\n",
        "dataset_tr = dataset_tr.map(lambda item1, item2, item3: tf.numpy_function(\n",
        "    map_func, [item1, item2, item3], [tf.float32, tf.int32, tf.float32]),\n",
        "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "dataset_tr = dataset_tr.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset_tr = dataset_tr.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((images_list_val, questions_val, answers_val))\n",
        "\n",
        "dataset_val = dataset_val.map(lambda item1, item2, item3: tf.numpy_function(\n",
        "    map_func, [item1, item2, item3], [tf.float32, tf.int32, tf.float32]),\n",
        "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "dataset_val = dataset_val.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset_val = dataset_val.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xffd1r_L-c_Q"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDU_76JG_cRy"
      },
      "source": [
        "### AttentionMaps Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93hdr7Ev9DhT"
      },
      "source": [
        "class AttentionMaps(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, dim_k, reg_value, **kwargs):\n",
        "    super(AttentionMaps, self).__init__(**kwargs)\n",
        "\n",
        "    self.dim_k = dim_k\n",
        "    self.reg_value = reg_value\n",
        "\n",
        "    self.Wv = Dense(self.dim_k, activation=None,\\\n",
        "                        kernel_regularizer=tf.keras.regularizers.l2(self.reg_value),\\\n",
        "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=2))\n",
        "    self.Wq = Dense(self.dim_k, activation=None,\\\n",
        "                        kernel_regularizer=tf.keras.regularizers.l2(self.reg_value),\\\n",
        "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=3))\n",
        "\n",
        "  def call(self, image_feat, ques_feat):\n",
        "\n",
        "    C = tf.matmul(ques_feat, tf.transpose(image_feat, perm=[0,2,1])) \n",
        "    C = tf.keras.activations.tanh(C) \n",
        "\n",
        "    WvV = self.Wv(image_feat)                             \n",
        "    WqQ = self.Wq(ques_feat)                              \n",
        "\n",
        "    WqQ_C = tf.matmul(tf.transpose(WqQ, perm=[0,2,1]), C) \n",
        "    WqQ_C = tf.transpose(WqQ_C, perm =[0,2,1])            \n",
        "\n",
        "    WvV_C = tf.matmul(tf.transpose(WvV, perm=[0,2,1]), tf.transpose(C, perm=[0,2,1]))  \n",
        "                        \n",
        "    WvV_C = tf.transpose(WvV_C, perm =[0,2,1])           \n",
        "\n",
        "    H_v = WvV + WqQ_C                                     \n",
        "    H_v = tf.keras.activations.tanh(H_v)                \n",
        "\n",
        "    H_q = WqQ + WvV_C                                     \n",
        "    H_q = tf.keras.activations.tanh(H_q)                  \n",
        "        \n",
        "    return [H_v, H_q]                                     \n",
        "  \n",
        "  def get_config(self):\n",
        " \n",
        "    config = {\n",
        "        'dim_k': self.dim_k,\n",
        "        'reg_value': self.reg_value\n",
        "    }\n",
        "    base_config = super(AttentionMaps, self).get_config()\n",
        "    return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GetHVi9s9JKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b85c686b-a7a2-4c2a-9817-a24d95174a9b"
      },
      "source": [
        "layer = AttentionMaps(64, 0.001)\n",
        "print(layer.get_config())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'name': 'attention_maps', 'trainable': True, 'dtype': 'float32', 'dim_k': 64, 'reg_value': 0.001}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjvDzU8aBR29"
      },
      "source": [
        "### ContextVector Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW_SkVP6BUwC"
      },
      "source": [
        "class ContextVector(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, reg_value, **kwargs):\n",
        "    super(ContextVector, self).__init__(**kwargs)\n",
        "\n",
        "    self.reg_value = reg_value\n",
        "\n",
        "    self.w_hv = Dense(1, activation='softmax',\\\n",
        "                        kernel_regularizer=tf.keras.regularizers.l2(self.reg_value),\\\n",
        "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=4))\n",
        "    self.w_hq = Dense(1, activation='softmax',\\\n",
        "                        kernel_regularizer=tf.keras.regularizers.l2(self.reg_value),\\\n",
        "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=5)) \n",
        "    \n",
        "\n",
        "  def call(self, image_feat, ques_feat, H_v, H_q):\n",
        "  \n",
        "    a_v = self.w_hv(H_v)                              \n",
        "    a_q = self.w_hq(H_q)                               \n",
        "\n",
        "    v = a_v * image_feat                              \n",
        "    v = tf.reduce_sum(v, 1)                           \n",
        "\n",
        "    q = a_q * ques_feat                               \n",
        "    q = tf.reduce_sum(q, 1)                          \n",
        "\n",
        "    return [v, q]\n",
        "\n",
        "  def get_config(self):\n",
        "\n",
        "    config = {\n",
        "        'reg_value': self.reg_value\n",
        "    }\n",
        "    base_config = super(ContextVector, self).get_config()\n",
        "    return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxylmEHBBaAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "159ccd52-97a3-44d3-d7d6-9e18f4935bcb"
      },
      "source": [
        "layer = ContextVector(0.001)\n",
        "print(layer.get_config())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'name': 'context_vector', 'trainable': True, 'dtype': 'float32', 'reg_value': 0.001}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bklQ3oDt_hsg"
      },
      "source": [
        "### PhraseLevelFeatures Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFlhybYu9qxs"
      },
      "source": [
        "class PhraseLevelFeatures(tf.keras.layers.Layer):\n",
        " \n",
        "  def __init__(self, dim_d, **kwargs):\n",
        "    super(PhraseLevelFeatures, self).__init__(**kwargs)\n",
        "    \n",
        "    self.dim_d = dim_d\n",
        "    \n",
        "    self.conv_unigram = Conv1D(self.dim_d, kernel_size=1, strides=1,\\\n",
        "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=6)) \n",
        "    self.conv_bigram =  Conv1D(self.dim_d, kernel_size=2, strides=1, padding='same',\\\n",
        "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=7)) \n",
        "    self.conv_trigram = Conv1D(self.dim_d, kernel_size=3, strides=1, padding='same',\\\n",
        "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=8)) \n",
        "\n",
        "\n",
        "  def call(self, word_feat):\n",
        " \n",
        "    x_uni = self.conv_unigram(word_feat)                  \n",
        "\n",
        "    x_bi  = self.conv_bigram(word_feat)                    \n",
        "\n",
        "    x_tri = self.conv_trigram(word_feat)                  \n",
        "\n",
        "    x = tf.concat([tf.expand_dims(x_uni, -1),\\\n",
        "                    tf.expand_dims(x_bi, -1),\\\n",
        "                    tf.expand_dims(x_tri, -1)], -1)       \n",
        "    x = tf.reduce_max(x, -1)                                \n",
        "\n",
        "    return x\n",
        "\n",
        "  def get_config(self):\n",
        " \n",
        "    config = {\n",
        "        'dim_d': self.dim_d\n",
        "    }\n",
        "    base_config = super(PhraseLevelFeatures, self).get_config()\n",
        "    return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33IIB3tK9sH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df4e627a-c84a-4ee6-97df-b3b5d5f9f3eb"
      },
      "source": [
        "layer = PhraseLevelFeatures(32)\n",
        "\n",
        "print(layer.get_config())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'name': 'phrase_level_features', 'trainable': True, 'dtype': 'float32', 'dim_d': 32}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESZZKbo1_pTU"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEVMYZhY-AU8"
      },
      "source": [
        "layer = None\n",
        "new_layer = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spizfuja9_mi"
      },
      "source": [
        "def build_model(num_answers, max_seq_len, vocab_size, dim_d, dim_k, l_rate, d_rate, reg_value):\n",
        "    \"\"\"\n",
        "    Defines the Keras model.\n",
        "\n",
        "    Arguments\n",
        "    ----------\n",
        "    max_answers : Number of output targets of the model.\n",
        "    max_seq_len : Length of input sequences\n",
        "    vocab_size  : Size of the vocabulary, i.e. maximum integer index + 1.\n",
        "    dim_d       : Hidden dimension\n",
        "    dim_k       : Hidden attention dimension\n",
        "    l_rate      : Learning rate for the model\n",
        "    d_rate      : Dropout rate\n",
        "    reg_value   : Regularization value\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    Returns the Keras model.\n",
        "    \"\"\"\n",
        "\n",
        "    image_input = Input(shape=(60, 512, ), name='Image_Input')\n",
        "    ques_input = Input(shape=(max_seq_len, ), name='Question_Input')\n",
        "\n",
        "    image_feat = Dense(dim_d, activation=None, name='Image_Feat_Dense',\\\n",
        "                            kernel_regularizer=tf.keras.regularizers.l2(reg_value),\\\n",
        "                                kernel_initializer=tf.keras.initializers.glorot_uniform(seed=1))(image_input)\n",
        "    image_feat = Dropout(d_rate, seed=1)(image_feat)\n",
        "\n",
        "    ques_feat_w = Embedding(input_dim=vocab_size, output_dim=dim_d, input_length=max_seq_len,\\\n",
        "                            mask_zero=True)(ques_input)\n",
        "    \n",
        "    Hv_w, Hq_w = AttentionMaps(dim_k, reg_value, name='AttentionMaps_Word')(image_feat, ques_feat_w)\n",
        "    v_w, q_w = ContextVector(reg_value, name='ContextVector_Word')(image_feat, ques_feat_w, Hv_w, Hq_w)\n",
        "    feat_w = tf.add(v_w,q_w)\n",
        "    h_w = Dense(dim_d, activation='tanh', name='h_w_Dense',\\\n",
        "                    kernel_regularizer=tf.keras.regularizers.l2(reg_value),\\\n",
        "                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=13))(feat_w)\n",
        "\n",
        "    ques_feat_p = PhraseLevelFeatures(dim_d, name='PhraseLevelFeatures')(ques_feat_w)\n",
        "\n",
        "    Hv_p, Hq_p = AttentionMaps(dim_k, reg_value, name='AttentionMaps_Phrase')(image_feat, ques_feat_p)\n",
        "    v_p, q_p = ContextVector(reg_value, name='ContextVector_Phrase')(image_feat, ques_feat_p, Hv_p, Hq_p)\n",
        "    feat_p = concatenate([tf.add(v_p,q_p), h_w], -1) \n",
        "    h_p = Dense(dim_d, activation='tanh', name='h_p_Dense',\\\n",
        "                    kernel_regularizer=tf.keras.regularizers.l2(reg_value),\\\n",
        "                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=14))(feat_p)\n",
        "\n",
        "    ques_feat_s = LSTM(dim_d, return_sequences=True, input_shape=(None, max_seq_len, dim_d),\\\n",
        "                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=16))(ques_feat_p)\n",
        "\n",
        "    Hv_s, Hq_s = AttentionMaps(dim_k, reg_value, name='AttentionMaps_Sent')(image_feat, ques_feat_s)\n",
        "    v_s, q_s = ContextVector(reg_value, name='ContextVector_Sent')(image_feat, ques_feat_p, Hv_s, Hq_s)\n",
        "    feat_s = concatenate([tf.add(v_s,q_s), h_p], -1) \n",
        "    h_s = Dense(2*dim_d, activation='tanh', name='h_s_Dense',\\\n",
        "                    kernel_regularizer=tf.keras.regularizers.l2(reg_value),\\\n",
        "                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=15))(feat_s)\n",
        "\n",
        "    z   = Dense(2*dim_d, activation='tanh', name='z_Dense',\\\n",
        "                    kernel_regularizer=tf.keras.regularizers.l2(reg_value),\\\n",
        "                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=16))(h_s)\n",
        "    z   = Dropout(d_rate, seed=16)(z)\n",
        "\n",
        "    result = Dense(num_answers, activation='softmax')(z)\n",
        "\n",
        "    model = Model(inputs=[image_input, ques_input], outputs=result)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQV50EL2-IIs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2faeda6d-b202-422f-c27a-b2d433fe08e3"
      },
      "source": [
        "vocab_size  = len(tok.word_index) + 1\n",
        "\n",
        "dim_d       = 512\n",
        "dim_k       = 256\n",
        "l_rate      = 1e-4\n",
        "d_rate      = 0.5\n",
        "reg_value   = 0.01\n",
        "  \n",
        "model = build_model(num_answers, MAX_QUESTIONS_LEN, vocab_size, dim_d, dim_k, l_rate, d_rate, reg_value)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Image_Input (InputLayer)        [(None, 60, 512)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Image_Feat_Dense (Dense)        (None, 60, 512)      262656      Image_Input[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Question_Input (InputLayer)     [(None, 21)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 60, 512)      0           Image_Feat_Dense[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 21, 512)      2377728     Question_Input[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "PhraseLevelFeatures (PhraseLeve (None, 21, 512)      1574400     embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "AttentionMaps_Word (AttentionMa [(None, 60, 256), (N 262656      dropout[0][0]                    \n",
            "                                                                 embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "AttentionMaps_Phrase (Attention [(None, 60, 256), (N 262656      dropout[0][0]                    \n",
            "                                                                 PhraseLevelFeatures[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "ContextVector_Word (ContextVect [(None, 512), (None, 514         dropout[0][0]                    \n",
            "                                                                 embedding[0][0]                  \n",
            "                                                                 AttentionMaps_Word[0][0]         \n",
            "                                                                 AttentionMaps_Word[0][1]         \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 21, 512)      2099200     PhraseLevelFeatures[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "ContextVector_Phrase (ContextVe [(None, 512), (None, 514         dropout[0][0]                    \n",
            "                                                                 PhraseLevelFeatures[0][0]        \n",
            "                                                                 AttentionMaps_Phrase[0][0]       \n",
            "                                                                 AttentionMaps_Phrase[0][1]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.add (TFOpLambda)        (None, 512)          0           ContextVector_Word[0][0]         \n",
            "                                                                 ContextVector_Word[0][1]         \n",
            "__________________________________________________________________________________________________\n",
            "AttentionMaps_Sent (AttentionMa [(None, 60, 256), (N 262656      dropout[0][0]                    \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.add_1 (TFOpLambda)      (None, 512)          0           ContextVector_Phrase[0][0]       \n",
            "                                                                 ContextVector_Phrase[0][1]       \n",
            "__________________________________________________________________________________________________\n",
            "h_w_Dense (Dense)               (None, 512)          262656      tf.math.add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "ContextVector_Sent (ContextVect [(None, 512), (None, 514         dropout[0][0]                    \n",
            "                                                                 PhraseLevelFeatures[0][0]        \n",
            "                                                                 AttentionMaps_Sent[0][0]         \n",
            "                                                                 AttentionMaps_Sent[0][1]         \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 1024)         0           tf.math.add_1[0][0]              \n",
            "                                                                 h_w_Dense[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.add_2 (TFOpLambda)      (None, 512)          0           ContextVector_Sent[0][0]         \n",
            "                                                                 ContextVector_Sent[0][1]         \n",
            "__________________________________________________________________________________________________\n",
            "h_p_Dense (Dense)               (None, 512)          524800      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 1024)         0           tf.math.add_2[0][0]              \n",
            "                                                                 h_p_Dense[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "h_s_Dense (Dense)               (None, 1024)         1049600     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "z_Dense (Dense)                 (None, 1024)         1049600     h_s_Dense[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 1024)         0           z_Dense[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 58)           59450       dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 10,049,600\n",
            "Trainable params: 10,049,600\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "246QU2MiB3-8"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1gBud2qBpC6"
      },
      "source": [
        "### Configure Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5z4upE2CBz46"
      },
      "source": [
        "training_dir = dataset_dir / 'temps'\n",
        "if not os.path.exists(training_dir):\n",
        "    os.mkdir(training_dir)\n",
        "\n",
        "checkpoints_dir = training_dir / ('checkpoint_'+str(l_rate)+\"_\"+str(dim_k))\n",
        "if not os.path.exists(checkpoints_dir):\n",
        "    os.mkdir(checkpoints_dir)\n",
        "\n",
        "train_log_dir = training_dir / 'logs' / (str(l_rate)+\"_\"+str(dim_k)) / 'train'\n",
        "val_log_dir   = training_dir / 'logs'/ (str(l_rate)+\"_\"+str(dim_k)) / 'validation'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJF8GdAcCm3z"
      },
      "source": [
        "EPOCHS          = 50\n",
        "\n",
        "steps_per_epoch = int(np.ceil(len(images_list_tr)/BATCH_SIZE))\n",
        "boundaries      = [EPOCHS * steps_per_epoch]\n",
        "values          = [l_rate, l_rate / 10]\n",
        "\n",
        "learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n",
        "optimizer        = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
        "\n",
        "loss_object      = tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction='auto')\n",
        "\n",
        "ckpt = tf.train.Checkpoint(step=tf.Variable(0), optimizer=optimizer, model=model)\n",
        "\n",
        "manager = tf.train.CheckpointManager(ckpt, checkpoints_dir, max_to_keep=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mnNE2iRDffP"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
        "val_loss = tf.keras.metrics.Mean('val_loss', dtype=tf.float32)\n",
        "\n",
        "train_score = F1Score(num_classes=num_answers, average='micro', name='train_score')\n",
        "val_score = F1Score(num_classes=num_answers, average='micro', name='val_score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJemZpzRD8Jm"
      },
      "source": [
        "train_summary_writer = tf.summary.create_file_writer(str(train_log_dir))\n",
        "val_summary_writer = tf.summary.create_file_writer(str(val_log_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DBVVfapE-SQ"
      },
      "source": [
        "### Restore Training Status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ-8eIAsCutI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c72d9c52-5fbc-4b3a-aa43-aa8283aa38bb"
      },
      "source": [
        "SAVE_CKPT_FREQ = 5\n",
        "\n",
        "if manager.latest_checkpoint:\n",
        "    ckpt.restore(manager.latest_checkpoint)\n",
        "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
        "    START_EPOCH = int(manager.latest_checkpoint.split('-')[-1]) * SAVE_CKPT_FREQ\n",
        "    print(\"Resume training from epoch: {}\".format(START_EPOCH))\n",
        "else:\n",
        "    print(\"Initializing from scratch\")\n",
        "    START_EPOCH = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Restored from /content/VQA_Dataset/temps/checkpoint_0.0001_256/ckpt-4\n",
            "Resume training from epoch: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VjxFQqrEeNe"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYqY28fDEdH-"
      },
      "source": [
        "def train_step(model, img, ques, ans, optimizer):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model([img, ques], training=True)\n",
        "    loss = loss_object(ans, predictions)\n",
        "\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_score(ans, predictions)\n",
        "\n",
        "  grads_ = list(zip(grads, model.trainable_variables))\n",
        "  return grads_\n",
        "\n",
        "def test_step(model, img, ques, ans):\n",
        "  predictions = model([img, ques])\n",
        "  loss = loss_object(ans, predictions)\n",
        "\n",
        "  val_loss(loss)\n",
        "  val_score(ans, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW6Ynu2NFH07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6710bda-d199-424a-a340-2cc2c885e9b9"
      },
      "source": [
        "for epoch in range(START_EPOCH, EPOCHS):\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  for img, ques, ans in (dataset_tr):\n",
        "    grads = train_step(model, img, ques, ans, optimizer)\n",
        "\n",
        "  with train_summary_writer.as_default():\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('f1_score', train_score.result(), step=epoch)\n",
        "    for var in model.trainable_variables:\n",
        "        tf.summary.histogram(var.name, var, step=epoch)\n",
        "    for grad, var in grads:\n",
        "        tf.summary.histogram(var.name + '/gradient', grad, step=epoch)\n",
        "\n",
        "  for img, ques, ans in (dataset_val):\n",
        "    test_step(model, img, ques, ans)\n",
        "  \n",
        "  with val_summary_writer.as_default():\n",
        "    tf.summary.scalar('loss', val_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('f1_score', val_score.result(), step=epoch)\n",
        "  \n",
        "  template = 'Epoch {}, loss: {:.4f}, f1_score: {:.4f}, val loss: {:.4f}, val f1_score: {:.4f}, time: {:.0f} sec'\n",
        "  print(template.format(epoch + 1,\n",
        "                         train_loss.result(), \n",
        "                         train_score.result(),\n",
        "                         val_loss.result(), \n",
        "                         val_score.result(),\n",
        "                         (time.time() - start)))\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_score.reset_states()\n",
        "  val_loss.reset_states()\n",
        "  val_score.reset_states()\n",
        "\n",
        "  ckpt.step.assign_add(1)\n",
        "  if int(ckpt.step) % SAVE_CKPT_FREQ == 0:\n",
        "      manager.save()\n",
        "      print('Saved checkpoint.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 21, loss: 2.2812, f1_score: 0.3612, val loss: 2.1987, val f1_score: 0.3771, time: 61 sec\n",
            "Epoch 22, loss: 2.2416, f1_score: 0.3649, val loss: 2.1739, val f1_score: 0.3829, time: 61 sec\n",
            "Epoch 23, loss: 2.2158, f1_score: 0.3706, val loss: 2.1486, val f1_score: 0.3840, time: 61 sec\n",
            "Epoch 24, loss: 2.2018, f1_score: 0.3701, val loss: 2.1566, val f1_score: 0.3823, time: 60 sec\n",
            "Epoch 25, loss: 2.1925, f1_score: 0.3746, val loss: 2.1353, val f1_score: 0.3824, time: 60 sec\n",
            "Saved checkpoint.\n",
            "Epoch 26, loss: 2.1803, f1_score: 0.3717, val loss: 2.1281, val f1_score: 0.3868, time: 61 sec\n",
            "Epoch 27, loss: 2.1665, f1_score: 0.3742, val loss: 2.1193, val f1_score: 0.3824, time: 60 sec\n",
            "Epoch 28, loss: 2.1429, f1_score: 0.3725, val loss: 2.0844, val f1_score: 0.3846, time: 60 sec\n",
            "Epoch 29, loss: 2.1066, f1_score: 0.3763, val loss: 2.0581, val f1_score: 0.3911, time: 60 sec\n",
            "Epoch 30, loss: 2.0624, f1_score: 0.3825, val loss: 1.9549, val f1_score: 0.4024, time: 60 sec\n",
            "Saved checkpoint.\n",
            "Epoch 31, loss: 2.0003, f1_score: 0.3959, val loss: 1.9040, val f1_score: 0.4073, time: 61 sec\n",
            "Epoch 32, loss: 1.9272, f1_score: 0.4017, val loss: 1.8408, val f1_score: 0.4149, time: 61 sec\n",
            "Epoch 33, loss: 1.8618, f1_score: 0.4067, val loss: 1.7980, val f1_score: 0.4194, time: 61 sec\n",
            "Epoch 34, loss: 1.8195, f1_score: 0.4096, val loss: 1.7253, val f1_score: 0.4205, time: 60 sec\n",
            "Epoch 35, loss: 1.7678, f1_score: 0.4163, val loss: 1.6981, val f1_score: 0.4207, time: 60 sec\n",
            "Saved checkpoint.\n",
            "Epoch 36, loss: 1.7300, f1_score: 0.4155, val loss: 1.6596, val f1_score: 0.4258, time: 61 sec\n",
            "Epoch 37, loss: 1.6960, f1_score: 0.4212, val loss: 1.6255, val f1_score: 0.4297, time: 60 sec\n",
            "Epoch 38, loss: 1.6589, f1_score: 0.4245, val loss: 1.6141, val f1_score: 0.4296, time: 61 sec\n",
            "Epoch 39, loss: 1.6411, f1_score: 0.4269, val loss: 1.5820, val f1_score: 0.4392, time: 60 sec\n",
            "Epoch 40, loss: 1.6234, f1_score: 0.4273, val loss: 1.5624, val f1_score: 0.4287, time: 61 sec\n",
            "Saved checkpoint.\n",
            "Epoch 41, loss: 1.5953, f1_score: 0.4308, val loss: 1.5477, val f1_score: 0.4391, time: 61 sec\n",
            "Epoch 42, loss: 1.5825, f1_score: 0.4320, val loss: 1.5332, val f1_score: 0.4400, time: 60 sec\n",
            "Epoch 43, loss: 1.5693, f1_score: 0.4337, val loss: 1.5233, val f1_score: 0.4368, time: 61 sec\n",
            "Epoch 44, loss: 1.5555, f1_score: 0.4367, val loss: 1.5170, val f1_score: 0.4425, time: 60 sec\n",
            "Epoch 45, loss: 1.5439, f1_score: 0.4379, val loss: 1.5099, val f1_score: 0.4435, time: 60 sec\n",
            "Saved checkpoint.\n",
            "Epoch 46, loss: 1.5305, f1_score: 0.4389, val loss: 1.5201, val f1_score: 0.4428, time: 60 sec\n",
            "Epoch 47, loss: 1.5268, f1_score: 0.4396, val loss: 1.4788, val f1_score: 0.4425, time: 60 sec\n",
            "Epoch 48, loss: 1.5096, f1_score: 0.4411, val loss: 1.4812, val f1_score: 0.4419, time: 61 sec\n",
            "Epoch 49, loss: 1.4973, f1_score: 0.4416, val loss: 1.4655, val f1_score: 0.4472, time: 60 sec\n",
            "Epoch 50, loss: 1.4872, f1_score: 0.4440, val loss: 1.4805, val f1_score: 0.4454, time: 61 sec\n",
            "Saved checkpoint.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsOQ59l3GNXe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWJ8NNJdGTYm"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvyk_4eVHcI-"
      },
      "source": [
        "### Create test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muNPOQ3rGV6b"
      },
      "source": [
        "tr_images = np.array(images_train)[TRAIN_INDEX.astype(int)]\n",
        "val_images = np.array(images_train)[VAL_INDEX.astype(int)]\n",
        "\n",
        "tr_questions = np.array(questions_train_processed)[TRAIN_INDEX.astype(int)]\n",
        "val_questions = np.array(questions_train_processed)[VAL_INDEX.astype(int)]\n",
        "\n",
        "tr_answers = np.array(answers_train)[TRAIN_INDEX.astype(int)]\n",
        "val_answers = np.array(answers_train)[VAL_INDEX.astype(int)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOvUQJCuG9Du"
      },
      "source": [
        "def map_func_eval(img_name, ques):\n",
        "    img_name = img_name.decode(\"utf-8\")\n",
        "    img_path = img_name.replace('Images','features')\n",
        "    img_path = img_path.replace('png','npy')\n",
        "    img_tensor = np.load(img_path)\n",
        "    return img_tensor, ques"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq0p9C_EHKqZ"
      },
      "source": [
        "dataset_tr_eval = tf.data.Dataset.from_tensor_slices((images_list_tr, questions_tr))\n",
        "\n",
        "dataset_tr_eval = dataset_tr_eval.map(lambda item1, item2: tf.numpy_function(\n",
        "    map_func_eval, [item1, item2], [tf.float32, tf.int32]),\n",
        "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "dataset_tr_eval = dataset_tr_eval.batch(BATCH_SIZE)\n",
        "\n",
        "dataset_val_eval = tf.data.Dataset.from_tensor_slices((images_list_val, questions_val))\n",
        "\n",
        "dataset_val_eval = dataset_val_eval.map(lambda item1, item2: tf.numpy_function(\n",
        "    map_func_eval, [item1, item2], [tf.float32, tf.int32]),\n",
        "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "dataset_val_eval = dataset_val_eval.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knvMPSBRHfwR"
      },
      "source": [
        "### Predict answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M12NiIw8HoV_"
      },
      "source": [
        "def preprocess_data(image_id, question):\n",
        "\n",
        "  features = np.load(str(features_dir / (image_id+'.npy')))\n",
        "  features = np.expand_dims(features,axis=0)\n",
        "  \n",
        "  processed_question = process_sentence(question)\n",
        "  tok.fit_on_texts(processed_question)\n",
        "  processed_question = [processed_question]\n",
        "  processed_question = tok.texts_to_sequences(processed_question)\n",
        "  processed_question = sequence.pad_sequences(processed_question, maxlen=MAX_QUESTIONS_LEN, padding='post')\n",
        "\n",
        "  return features,processed_question\n",
        "\n",
        "def predict(image_feature, processed_question):\n",
        "  answer = model([image_feature,processed_question])\n",
        "  answer = tf.argmax(answer, axis=1, output_type=tf.int32)\n",
        "  answer = (labelencoder.inverse_transform(answer))\n",
        "  return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RafLkuQNHtGX"
      },
      "source": [
        "test_questions = json.load(open(dataset_dir / 'test_questions.json', 'r'))\n",
        "\n",
        "labels_dict = {\n",
        "        '0': 0,\n",
        "        '1': 1,\n",
        "        '2': 2,\n",
        "        '3': 3,\n",
        "        '4': 4,\n",
        "        '5': 5,\n",
        "        'apple': 6,\n",
        "        'baseball': 7,\n",
        "        'bench': 8,\n",
        "        'bike': 9,\n",
        "        'bird': 10,\n",
        "        'black': 11,\n",
        "        'blanket': 12,\n",
        "        'blue': 13,\n",
        "        'bone': 14,\n",
        "        'book': 15,\n",
        "        'boy': 16,\n",
        "        'brown': 17,\n",
        "        'cat': 18,\n",
        "        'chair': 19,\n",
        "        'couch': 20,\n",
        "        'dog': 21,\n",
        "        'floor': 22,\n",
        "        'food': 23,\n",
        "        'football': 24,\n",
        "        'girl': 25,\n",
        "        'grass': 26,\n",
        "        'gray': 27,\n",
        "        'green': 28,\n",
        "        'left': 29,\n",
        "        'log': 30,\n",
        "        'man': 31,\n",
        "        'monkey bars': 32,\n",
        "        'no': 33,\n",
        "        'nothing': 34,\n",
        "        'orange': 35,\n",
        "        'pie': 36,\n",
        "        'plant': 37,\n",
        "        'playing': 38,\n",
        "        'red': 39,\n",
        "        'right': 40,\n",
        "        'rug': 41,\n",
        "        'sandbox': 42,\n",
        "        'sitting': 43,\n",
        "        'sleeping': 44,\n",
        "        'soccer': 45,\n",
        "        'squirrel': 46,\n",
        "        'standing': 47,\n",
        "        'stool': 48,\n",
        "        'sunny': 49,\n",
        "        'table': 50,\n",
        "        'tree': 51,\n",
        "        'watermelon': 52,\n",
        "        'white': 53,\n",
        "        'wine': 54,\n",
        "        'woman': 55,\n",
        "        'yellow': 56,\n",
        "        'yes': 57\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmc-rD7YI4Xx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cead745a-bb4a-4fd6-f76b-9eee571a495f"
      },
      "source": [
        "results = {}\n",
        "\n",
        "for key, value in tqdm(test_questions.items()):\n",
        "    image_features, processed_question = preprocess_data(value['image_id'],value['question'])\n",
        "    answer = predict(image_features,processed_question)\n",
        "    \n",
        "    results[key] = labels_dict.get(answer[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6372/6372 [03:23<00:00, 31.34it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2yr_C9DI913"
      },
      "source": [
        "### Create CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0KjVaGcI_h7"
      },
      "source": [
        "def create_csv(results, results_dir='./'):\n",
        "\n",
        "    csv_fname = 'results_'\n",
        "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
        "\n",
        "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
        "\n",
        "        f.write('Id,Category\\n')\n",
        "\n",
        "        for key, value in results.items():\n",
        "            f.write(str(key) + ',' + str(value) + '\\n')\n",
        "\n",
        "create_csv(results, str(dataset_dir))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}